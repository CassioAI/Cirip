{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ColorNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Dq6NX4CCAqAD",
        "colab_type": "code",
        "outputId": "a7073b4d-be6f-4525-f7a9-a5a9086bc6e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Importing the used packages\n",
        "## Numpy for arrays\n",
        "\n",
        "import numpy as np\n",
        "## Pyplot from Matplotlib for vizualising the test results in plots\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "## Drive from Google.Colab because I was storing my database in google drive\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras import backend as k\n",
        "## Keras.* for building my fully connected dense neural network \n",
        "from keras.models import  save_model, load_model, Model\n",
        "from keras.optimizers import Adadelta\n",
        "\n",
        "## keras.callbacks.* for Earlystopping (modelcheckpoint is needed to load back the weights)\n",
        "from keras.callbacks import Callback\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.activations import relu as Relu\n",
        "from keras.activations import sigmoid as Sigmoid\n",
        "from keras.layers import UpSampling2D, Input, concatenate, BatchNormalization\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "## Mean_squared_error from sklearn.metrics for calculating mean squared error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import skimage.color as skcolor\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "## Math for Sqrt\n",
        "import math\n",
        "import h5py\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "import datetime\n",
        "import scipy\n",
        "import random\n",
        "\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from timeit import default_timer as timer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "n0Kp_GMYBbWw",
        "colab_type": "code",
        "outputId": "c830f0f8-3b2f-4293-dd25-61619698763d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vIW2yv6M-6K0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function padding the int to the given size\n",
        "## length:           int, the length of\n",
        "## return value:     String, %0(length)d\n",
        "def formatint(length):\n",
        "  return str('%0' + str('%d'% length)+'d')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2dP8lhxkAw-y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Function for making folder if it does not exist in the given path.\n",
        "#@ Used packages:\n",
        "  #@ os\n",
        "## path:     String, filepath needed to be verified\n",
        "def MakePath(path):\n",
        "  #Creating folder if it does not exist.\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cf8hI-uyAx66",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Function for printing text and the time\n",
        "#@ Used packages:\n",
        "  #@ datetime\n",
        "## string:        String, the printed text\n",
        "## return value:  datetime, the printings time\n",
        "def PrintTime(string):\n",
        "    time = datetime.datetime.now()\n",
        "    print(string+ str(time));\n",
        "    return time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCKZQDy8A46b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Class for visualizing\n",
        "#@ Used packages:\n",
        "  #@ numpy as np\n",
        "  #@ os\n",
        "  #@ from PIL Image\n",
        "  #@ skimage.Color as skcolor\n",
        "class Visualization():\n",
        "    # Function for converting RGB image array to grayscale image array.\n",
        "    ## imageArray:               np.array, (None,None,3) RGB\n",
        "    ## return value:             np.array, Grayscaled image\n",
        "    def img_grayscale(self, imageArray):\n",
        "        imgArr = np.empty([1])\n",
        "        for image in imageArray:\n",
        "            pil_imgray = image.convert('LA')\n",
        "            img        = np.array(list(pil_imgray.getdata(band = 0)), int)\n",
        "            img.shape  = (pil_imgray.size[1], pil_imgray.size[0])\n",
        "            imgArr     = np.append(imgArr, img)\n",
        "        imgArr = np.delete(imgArr,0)\n",
        "        imgArr.shape = (len(imageArray),pil_imgray.size[1], pil_imgray.size[0])\n",
        "        return imgArr;\n",
        "      \n",
        "    # Function for making visualization for the images.\n",
        "    ## transformed_dataset_path:   String, folder path of transformed dataset\n",
        "    ## width:                      int, number of images on one edge\n",
        "    ## size:                       int, size of 1:1 ratio image\n",
        "    def image_mosaic(self, transformed_dataset_path, width, size):\n",
        "\n",
        "        ## Making a list of filenames of the dataset directory.\n",
        "        filename_list = os.listdir(transformed_dataset_path)\n",
        "        ## Creating an empty list for the loaded images.\n",
        "        imagearray = []\n",
        "        ## Loading the first width*width number of images, and appending them to the list.\n",
        "        for i in range(width*width):\n",
        "            im = Image.open(transformed_dataset_path + filename_list[i])\n",
        "            imagearray.append(im)\n",
        "\n",
        "        ## Initializing canvas.\n",
        "        canvas = np.ones((size * width, size * width * 2,3));\n",
        "        ## Resizing images.\n",
        "        for i in range(len(imagearray)):\n",
        "            imagearray[i]=imagearray[i].resize((size, size),Image.ANTIALIAS)\n",
        "        ## Making GrayImages.\n",
        "        grayimage = self.img_grayscale(imagearray)\n",
        "        ## Writing the RGB images to the canvas right side.\n",
        "        for i in range(width):\n",
        "            for j in range(width):\n",
        "                canvas[i * size:(i + 1) * size,j * size:(j + 1) * size, 0::] = np.array(imagearray[i + width * j])\n",
        "        ## Writing grayscale images to the canvas left side.\n",
        "        for i in range(width):\n",
        "            for j in range(width, 2 * width):\n",
        "                canvas[i * size:(i + 1) * size, j * size:(j + 1) * size, 0]=np.array(grayimage[i + width * (j - width)])\n",
        "                canvas[i * size:(i + 1) * size, j * size:(j + 1) * size, 1]=np.array(grayimage[i + width * (j - width)])\n",
        "                canvas[i * size:(i + 1) * size, j * size:(j + 1) * size, 2]=np.array(grayimage[i + width * (j - width)])\n",
        "\n",
        "        ## Displaying the mosaic.\n",
        "        canvas = canvas.astype(np.uint8);\n",
        "        mosaic = Image.fromarray(canvas)\n",
        "        mosaic.show()\n",
        "        \n",
        "    # Function\n",
        "    ## image:          np.array, (None, None,3) LAB image\n",
        "    ## idim:           integer, which dimension needed to be returned\n",
        "    ## return value:   np.array, (None,None,3) RGB image\n",
        "    def extract_single_dim_from_LAB_convert_to_RGB(self, image, idim):\n",
        "        ## Initializing z with input image shape\n",
        "        z = np.zeros(image.shape)\n",
        "        ## Need brightness to plot the image along 1st or 2nd axis\n",
        "        if idim != 0 :\n",
        "            z[:, :, 0] = 60     \n",
        "        z[:, :, idim] = image[:, :, idim]\n",
        "        z = skcolor.lab2rgb(z)\n",
        "        return(z)\n",
        "      \n",
        "    # Function\n",
        "    ## image:          np.array, (None, None,3) LAB image\n",
        "    ## return value:   np.array, (None,None,3) RGB image\n",
        "    def extract_AB_from_LAB_convert_to_RGB(self, image):\n",
        "        ## Initializing z with input image shape\n",
        "        z = np.zeros(image.shape)\n",
        "        ## Need brightness to plot the image along 1st or 2nd axis\n",
        "        z[:, :, 0] = 60\n",
        "        z[:, :, 1::] = image[:, :, 1::]\n",
        "        z = skcolor.lab2rgb(z)\n",
        "        return(z)\n",
        "      \n",
        "    # Function for visualizing LAB colorspace shows Lightness (grayscale), A, B, AB, LAB\n",
        "    ## lab:     np.array, (None, None, 3) LAB image\n",
        "    def LABColorVisualize(self, lab):\n",
        "        ## Initializing the plot\n",
        "        fig = plt.figure(figsize=(13,5))\n",
        "        count = 1 \n",
        "        \n",
        "        ax = fig.add_subplot(1,5,count)        \n",
        "        ## Converting Lightness to RGB\n",
        "        lab_rgb_gray = self.extract_single_dim_from_LAB_convert_to_RGB(lab,0) \n",
        "        ax.imshow(lab_rgb_gray); ax.axis(\"off\")\n",
        "        ax.set_title(\"L: lightness\")\n",
        "        count += 1\n",
        "        \n",
        "        ax = fig.add_subplot(1,5,count)\n",
        "        ## Converting A to RGB\n",
        "        lab_rgb_a = self.extract_single_dim_from_LAB_convert_to_RGB(lab,1) \n",
        "        ax.imshow(lab_rgb_a); ax.axis(\"off\")\n",
        "        ax.set_title(\"A: color spectrums green to red\")\n",
        "        count += 1\n",
        "\n",
        "        ax = fig.add_subplot(1,5,count)\n",
        "        ## Converting B to RGB\n",
        "        lab_rgb_b = self.extract_single_dim_from_LAB_convert_to_RGB(lab,2) \n",
        "        ax.imshow(lab_rgb_b); ax.axis(\"off\")\n",
        "        ax.set_title(\"B: color spectrums blue to yellow\")\n",
        "        count += 1\n",
        "\n",
        "        ax = fig.add_subplot(1,5,count)\n",
        "        ## Converting AB to RGB\n",
        "        lab_rgb_ab = self.extract_AB_from_LAB_convert_to_RGB(lab) \n",
        "        ax.imshow(lab_rgb_ab); ax.axis(\"off\")\n",
        "        ax.set_title(\"AB\")\n",
        "        count += 1\n",
        "        \n",
        "        ax = fig.add_subplot(1,5,count)\n",
        "        ## Converting LAB image to RGB\n",
        "        lab_rgb_lab = skcolor.lab2rgb(lab)\n",
        "        ax.imshow(lab_rgb_lab); ax.axis(\"off\")\n",
        "        ax.set_title(\"LAB\")\n",
        "        count += 1\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BZJw6HQO_7ch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for making the U-net model\n",
        "## input_size:      integer tuple, default value (128, 128, 1)\n",
        "## return value:    keras.model the u-net model.\n",
        "#@ Used packages:\n",
        "  #@ Conv2D\n",
        "  #@ Input\n",
        "  #@ BatchNormalization\n",
        "  #@ concatenate\n",
        "  #@ Upsampling2D\n",
        "  #@ Model\n",
        "def unet(input_size = (128, 128, 1)):\n",
        "    ## 2D convolutional layers parameters\n",
        "    Conv2D_param = {\n",
        "        'kernel_size'       : (3, 3),\n",
        "        'activation'        : 'relu',\n",
        "        'padding'           : 'same',\n",
        "        'kernel_initializer': 'he_normal'\n",
        "    }\n",
        "    '''\n",
        "    We used the default input_size values for represantating the network layers size.\n",
        "    '''\n",
        "  ## Encoder\n",
        "    ## Defining inputsize\n",
        "    inputs = Input(input_size)\n",
        "    \n",
        "    ## Convolutional layer with BatchNormalization (128, 128, 1) -> (64, 64, 64)\n",
        "    conv1 = Conv2D(64, strides = 2, **Conv2D_param)(inputs)\n",
        "    conv1 = BatchNormalization(axis = 3, momentum = 0.9)(conv1)\n",
        "    \n",
        "    ## Convolutional layer with BatchNormalization (64, 64, 64) -> (64, 64, 128)\n",
        "    conv2 = Conv2D(128, **Conv2D_param)(conv1)\n",
        "    conv2 = BatchNormalization(axis = 3, momentum = 0.9)(conv2)\n",
        "    \n",
        "    ## Convolutional layer with BatchNormalization (64, 64, 64) -> (32, 32, 128)\n",
        "    conv3 = Conv2D(128, strides = 2, **Conv2D_param)(conv2)\n",
        "    conv3 = BatchNormalization(axis = 3, momentum = 0.9)(conv3)\n",
        "    \n",
        "    ## Convolutional layer with BatchNormalization (32, 32, 128) -> (32, 32, 256)\n",
        "    conv4 = Conv2D(256, **Conv2D_param)(conv3)\n",
        "    conv4 = BatchNormalization(axis = 3, momentum = 0.9)(conv4)\n",
        "    \n",
        "    ## Convolutional layer with BatchNormalization (32, 32, 256) -> (16, 16, 256)\n",
        "    conv5 = Conv2D(256, strides = 2, **Conv2D_param)(conv4)\n",
        "    conv5 = BatchNormalization(axis = 3, momentum = 0.9)(conv5)\n",
        "    \n",
        "  ## Bottleneck\n",
        "    ## Convolutional layer with BatchNormalization (16, 16, 256) -> (16, 16, 512)\n",
        "    conv6 = Conv2D(512, **Conv2D_param)(conv5)\n",
        "    conv6 = BatchNormalization(axis = 3, momentum = 0.9)(conv6)\n",
        "    \n",
        "  ## Decoder\n",
        "    ## Concatenate two layers (conv5, conv6) (16, 16, 256 + 512)\n",
        "    merge7 = concatenate([conv5,conv6], axis = 3)\n",
        "    \n",
        "    ## UpSampling (32, 32, 768)\n",
        "    up7    = UpSampling2D(size = (2,2))(merge7)\n",
        "    \n",
        "    ## Convolutional layer with BatchNormalization (32, 32, 768) -> (32, 32, 256)\n",
        "    conv7  = Conv2D(256, **Conv2D_param)(up7)\n",
        "    conv7  = BatchNormalization(axis = 3, momentum = 0.9)(conv7)\n",
        "    \n",
        "    ## Concatenate two layers (conv4, conv7) (32, 32, 256 + 256)\n",
        "    merge8 = concatenate([conv4,conv7], axis = 3)\n",
        "    \n",
        "    ## Convolutional layer with BatchNormalization (32, 32, 512) -> (32, 32, 256)\n",
        "    conv8  = Conv2D(256, **Conv2D_param)(merge8)\n",
        "    conv8  = BatchNormalization(axis = 3, momentum = 0.9)(conv8)\n",
        "    \n",
        "    ## Concatenate two layers (conv3, conv8) (32, 32, 128 + 256)\n",
        "    merge9 = concatenate([conv3,conv8], axis = 3)\n",
        "    \n",
        "    ## UpSampling (64, 64, 384)\n",
        "    up9    = UpSampling2D(size = (2,2))(merge9)   \n",
        "    \n",
        "    ## Convolutional layer with BatchNormalization (64, 64, 384) -> (64, 64, 128)\n",
        "    conv9  = Conv2D(128, **Conv2D_param)(up9)\n",
        "    conv9  = BatchNormalization(axis = 3, momentum = 0.9)(conv9)\n",
        "    \n",
        "    ## Concatenate two layers (conv2, conv9) (64, 64, 128 + 128)\n",
        "    merge10 = concatenate([conv2,conv9], axis = 3)\n",
        "    \n",
        "    ## Convolutional layer with BatchNormalization (64, 64, 256) -> (64, 64, 128)\n",
        "    conv10  = Conv2D(128, **Conv2D_param)(merge10)\n",
        "    conv10  = BatchNormalization(axis = 3, momentum = 0.9)(conv10)\n",
        "    \n",
        "    ## Concatenate two layers (conv1, conv10) (64, 64, 64 + 128)\n",
        "    merge11 = concatenate([conv1,conv10], axis = 3)\n",
        "    \n",
        "    ## UpSampling (128, 128, 192)\n",
        "    up11    = UpSampling2D(size = (2,2))(merge11)\n",
        "    \n",
        "    ## Convolutional layer with BatchNormalization (128, 128, 192) -> (128, 128, 64)\n",
        "    conv11  = Conv2D(64, **Conv2D_param)(up11)\n",
        "    conv11  = BatchNormalization(axis = 3, momentum = 0.9)(conv11)\n",
        "    \n",
        "    ## Convolutional layer with BatchNormalization (128, 128, 64) -> (128, 128, 32)\n",
        "    conv12 = Conv2D(32, **Conv2D_param)(conv11)\n",
        "    conv12 = BatchNormalization(axis = 3, momentum = 0.9)(conv12)\n",
        "    \n",
        "    ## Convolutional layer with BatchNormalization and sigmoid acitvation (128, 128, 32) -> (128, 128, 2)\n",
        "    conv13 = Conv2D(2, kernel_size = (3,3),padding = 'same', activation = 'sigmoid')(conv12)\n",
        "    \n",
        "    ## Build model\n",
        "    model = Model(inputs = inputs, outputs = conv13)\n",
        "    \n",
        "    ## Return model\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kjl4n-PlA-6q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for PSNR loss \n",
        "def PSNRLoss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    PSNR is Peek Signal to Noise Ratio, which is similar to mean squared error.\n",
        "    It does not have a max value so we set our loss to 1/PSNR this way our loss is 0 when PSNR is infinity\n",
        "    \"\"\"\n",
        "    return 1/tf.image.psnr(y_true, y_pred,1)\n",
        "# Saving PSNRLoss so we can load our model\n",
        "get_custom_objects().update({\"PSNRLoss\": PSNRLoss})\n",
        "\n",
        "# Function for SSIM loss\n",
        "def ssim(y_true,y_pred):\n",
        "    ''' \n",
        "    SSIM is structural similarity index. Max value is 1 when to to inputid image is the same.\n",
        "    So for our loss we make 1-ssim. This way our loss is 0 when the images are the same\n",
        "    '''\n",
        "    return 1-tf.image.ssim(y_pred,y_true,1)\n",
        "get_custom_objects().update({\"ssim\": ssim})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jugWjqpT-fi3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for loading model\n",
        "## path:          string, the file path where the model is saved\n",
        "## model_name:    string, the name of the model\n",
        "## return value:  \n",
        "def loadModel(path, model_name, model_number = None ):\n",
        "    ## Setting model_name uppercase\n",
        "    model_name = model_name.upper() \n",
        "    ## Loading previous models\n",
        "    previous_models = [x for x in os.listdir(path) if model_name in x]\n",
        "    \n",
        "    if model_number != None:\n",
        "      if len(previous_models) >= model_number:\n",
        "        ## Print feedback\n",
        "        print('Loading model:', previous_models[model_number])\n",
        "        ## Return with the loaded model\n",
        "        return load_model(path + previous_models[model_number])\n",
        "      else:\n",
        "        ## Print feedback\n",
        "        print('Warning: There is no previous models with this name: ' + str(model_name) + ' or with this numner: ' + str(model_number))       \n",
        "    else:\n",
        "      ## Does not have previous models\n",
        "      if len(previous_models) == 0: \n",
        "          \n",
        "          ## Print feedback\n",
        "          print('Warning: There is no previous models with this name:', model_name)\n",
        "          \n",
        "      ## Does have previous models\n",
        "      else: \n",
        "          ## Print feedback\n",
        "          print('Loading model:', previous_models[-1])\n",
        "          ## return with the loaded model\n",
        "          return load_model(path + previous_models[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sh7ADmYtF1Gf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for iterating through the database\n",
        "## data_h5df:           h5df, the dataset\n",
        "## batch_size:          integer, the batch size\n",
        "## random_seed:         integer, seed for initializing random\n",
        "## return value:        (X_batch, Y_batch) Scaled input and output for the network (batch_size, 128, 128, 1) (batch_size, 128, 128, 2)\n",
        "#@ Used Packages:\n",
        "  #@ numpy as np\n",
        "def DataGenerator(data_h5df, batch_size = 64, random_seed = 1):\n",
        "  ## Array of indices\n",
        "  indices = list(data_h5df.keys())\n",
        "  ## Infiniti generator\n",
        "  while True:\n",
        "        ## Setting random seed\n",
        "        np.random.seed(random_seed) \n",
        "        ## Shuffle indices before evry epoch\n",
        "        np.random.shuffle(indices)\n",
        "        ## Iterating through all the elements\n",
        "        for i in range(0, len(indices) - batch_size + 1, batch_size):\n",
        "            ## Batch indices , batch_size the size\n",
        "            batch_indices = indices[i:i + batch_size]\n",
        "            ## Placeholder for the LAB image data\n",
        "            lab_batch = np.empty((batch_size,*np.array(val[batch_indices[0]]).shape))\n",
        "            ## Saving the batch number of image data\n",
        "            for iterator in range(batch_size):\n",
        "              lab_batch[iterator] = np.array(data_h5df[batch_indices[iterator]]).astype('float32')\n",
        "            ## Scaling inputs /100\n",
        "            X_batch = ((lab_batch[:,:,:,0:1])/100.0).astype('float32')\n",
        "            ## Scaling outputs +128 / 255\n",
        "            Y_batch = ((lab_batch[:,:,:,1:3] + 128.0 ) / 255.0).astype('float32')\n",
        "            ## Yielding batch\n",
        "            yield (X_batch,Y_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L4UihEU3BFGi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Class for a custom Callback\n",
        "#@ Used Packages:\n",
        "  #@ os\n",
        "  #@ numpy as np\n",
        "#@ Used functions:\n",
        "  #@ MakePath\n",
        "  #@ fomatint\n",
        "class WeightsSaver(Callback):\n",
        "    ## Initializing the class\n",
        "    def __init__(self, model_name,loss_name, save_path, preds_array, save_preds_per_n_batch):\n",
        "        ## Epoch, batch counter\n",
        "        self.epoch = 0\n",
        "        self.batch = 0\n",
        "        ## Model, loss name set to uppercase\n",
        "        self.model_name = model_name.upper()\n",
        "        self.loss_name = loss_name.upper()\n",
        "        ## Storing \n",
        "        self.N = save_preds_per_n_batch\n",
        "        self.path = save_path+'/'+model_name.upper() +'/'+ loss_name.upper()\n",
        "        self.model_path =  self.path + '/model/'\n",
        "        self.pred_path =  self.path + '/images/'\n",
        "        self.preds = preds_array\n",
        "        ## Making path for models and images\n",
        "        MakePath(save_path + '/' + model_name.upper() + '/')\n",
        "        MakePath(self.path)\n",
        "        MakePath(self.model_path)\n",
        "        MakePath(self.pred_path)\n",
        "        \n",
        "        ## Setting previous model name\n",
        "        previous_models = [n for n in os.listdir(self.model_path) if self.model_name in n]\n",
        "        if len(previous_models) == 0:\n",
        "          self.model_number = 0\n",
        "        else:\n",
        "          self.model_number = int(previous_models[-1][-10:-5])\n",
        "\n",
        "\n",
        "    ## Function called when epoch ends, save the model \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.batch = 0\n",
        "        self.epoch += 1\n",
        "        self.model_number += 1\n",
        "        \n",
        "        name = (self.model_path + self.model_name + '_' + formatint(5) + '.h5df') % self.model_number\n",
        "        save_model(self.model, name)\n",
        "    ## Function called when batch end, if N-th batch save the images\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.batch += 1 \n",
        "\n",
        "        if self.batch % self.N == 0:\n",
        "            y_pred = self.model.predict(self.preds[:,:,:,0].astype('float32').reshape(*(self.preds[:,:,:,0].shape),1)/100.0)*255-128\n",
        "            for iterator in range(y_pred.shape[0]):\n",
        "              name = self.path+'/images/pred%(pred_n)03d_image%(epoch)03d_%(batch)05d' % {'pred_n':iterator,'epoch':self.model_number,'batch':self.batch}\n",
        "              image=np.empty((128,128,3))\n",
        "              image[:,:,0]=self.preds[iterator,:,:,0]\n",
        "              image[:,:,1::]=y_pred[iterator]\n",
        "              image=skcolor.lab2rgb(image)\n",
        "              im=Image.fromarray((image*255).astype('uint8'))\n",
        "              im.save(name+'.jpg')    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oCJaTha6BK8Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Loading dataset\n",
        "data= h5py.File('/content/gdrive/My Drive/DeepLearning/images_hdf5/dataset.h5df', 'r') \n",
        "## Datasets\n",
        "train = data['train']\n",
        "val = data['val']\n",
        "## Setting up generators\n",
        "training_generator = DataGenerator(train,64)\n",
        "validation_generator = DataGenerator(val,64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SEcz6lQWABjJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Setting up model\n",
        "u_model = unet(input_size = (128,128,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5_ecc2w3vRMg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test = data['test']\n",
        "pred_test = np.empty((17,128,128,3))\n",
        "\n",
        "for iterator in range(17):\n",
        "  pred_test[iterator]=np.array(test[list(test.keys())[iterator]],dtype='float64')\n",
        "\n",
        "mc = WeightsSaver('customunet1','PSNRLoss','/content/gdrive/My Drive/DeepLearning', pred_test,100)\n",
        "\n",
        "model_params = {'generator': training_generator,\n",
        "                'steps_per_epoch': len(list(data['train'].keys()))//64,\n",
        "                'validation_steps':  len(list(data['val'].keys()))//64,\n",
        "                'validation_data': validation_generator,\n",
        "                'callbacks':[mc],\n",
        "                'epochs': 20\n",
        "               }\n",
        "u_model_load = loadModel(mc.model_path,'customunet1')\n",
        "if(u_model_load != None):\n",
        "  u_model = u_model_load\n",
        "\n",
        "u_model.compile(loss=PSNRLoss , metrics=['accuracy'],optimizer=Adadelta(lr=1, rho=0.95, epsilon=None, decay=0.0))\n",
        "# Train model on dataset\n",
        "u_model.fit_generator(**model_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "74LxLXqST4MS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_test = np.empty((17,128,128,3))\n",
        "\n",
        "for iterator in range(17):\n",
        "  pred_test[iterator]=np.array(test[list(test.keys())[iterator]],dtype='float64')\n",
        "mc = WeightsSaver('customunet','ssim', '/content/gdrive/My Drive/DeepLearning', pred_test,100)\n",
        "test = data['test']\n",
        "\n",
        "model_params = {'generator': training_generator,\n",
        "                'steps_per_epoch': len(list(data['train'].keys()))//64,\n",
        "                'validation_steps':  len(list(data['val'].keys()))//64,\n",
        "                'validation_data': validation_generator,\n",
        "                'callbacks':[mc],\n",
        "                'epochs': 6}\n",
        "u_model_load = loadModel(mc.model_path,'customunet')\n",
        "if(u_model_load != None):\n",
        "  u_model = u_model_load\n",
        "\n",
        "u_model.compile(loss=ssim , metrics=['accuracy'],optimizer=Adadelta(lr=1, rho=0.95, epsilon=None, decay=0.0))\n",
        "# Train model on dataset\n",
        "u_model.fit_generator(**model_params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hiIbxWxTLC3a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vi = Visualization()\n",
        "test = data['test']\n",
        "test_im = np.array(test[list(test.keys())[15]],dtype = 'float64')\n",
        "\n",
        "preds = (u_model.predict(test_im[:,:,0].reshape(1, 128, 128, 1) / 100) * 255 - 128)\n",
        "predicted_images          = np.empty((128, 128, 3))\n",
        "predicted_images[:, :, 0]   = test_im[:, :, 0]\n",
        "predicted_images[:, :, 1::] = preds\n",
        "vi.LABColorVisualize(predicted_images)\n",
        "\n",
        "real_images = np.empty((128,128,3))\n",
        "real_images = test_im\n",
        "vi.LABColorVisualize(real_images)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}