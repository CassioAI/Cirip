{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ColorNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CassioAI/Cirip/blob/master/ColorNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "s9qBI03FrcgL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  A short introduction"
      ]
    },
    {
      "metadata": {
        "id": "oCPtUo00rcgQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are working on a project to color grayscale pictures.\n",
        "The first few aspect of our project that we would like to introduce in this short introduction, are the decisions we took in our current approach to the problem.\n",
        "\n",
        "The Dataset:\n",
        "At first we were planning on using the ImageNet Database, but soon it became evident, that due to some technical difficulties on the site we weren't going to get access, so we decided on our current Database (Open Images V4 Dataset) which granted us the original images in a zip file. The images are not restricted to a single subject, the dataset incorporates images with various themes. We decided to start with 100000 images.\n",
        "(https://www.figure-eight.com/dataset/open-images-annotated-with-bounding-boxes/)\n",
        "\n",
        "The Images:\n",
        "Although the Images in the Database were mostly satisfactory for our goals, there were some modifications we had to make on them. Since the images were too big for us to handle and they did not have the same measurements we decided to crop them into a square shape and scale them down to resolution of 128X128. Also some of the pictures were not appropriate for our task such as grayscale pictures, so we decided, to sort them out.\n",
        "\n",
        "The LAB color scale:\n",
        "We decided to convert the images from RGB color space to LAB, which hopefully will make the teaching easier, as the L channel of LAB colorspace is the greyscale representation of the image, so the machine will only have to predict two channels instead of 3.\n",
        "\n",
        "The Small Parts:\n",
        "Even with a 128X128 scaling the whole dataset is too big to be loaded to the operating memory as a numpy array, so we decided to divide it into smaller parts, and teach the neural network on each smaller part individually."
      ]
    },
    {
      "metadata": {
        "id": "m79XKvNKrcgY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The preparation and preprocessing of the dataset"
      ]
    },
    {
      "metadata": {
        "id": "XesJ2Jrcrcgi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Importing the used packages\n",
        "# Numpy for arrays\n",
        "import numpy as np\n",
        "# requests for downloading the dataset\n",
        "import requests\n",
        "# PIL.Image for image processing\n",
        "from PIL import Image\n",
        "# keras.preprocessing.image for image processing\n",
        "import keras.preprocessing.image as k_image\n",
        "# os for file management\n",
        "import os\n",
        "# skimage.color for transforming the color model of images\n",
        "import skimage.color as skcolor\n",
        "# zipfile for extracting downloaded dataset \n",
        "import zipfile\n",
        "# random for random number generation in grayscale check\n",
        "import random\n",
        "\n",
        "import math\n",
        "\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "# Importing the used packages\n",
        "## Numpy for arrays\n",
        "import numpy as np\n",
        "## Pyplot from Matplotlib for vizualising the test results in plots\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "## Math for Sqrt\n",
        "import math\n",
        "## Drive from Google.Colab because I was storing my database in google drive\n",
        "from google.colab import drive\n",
        "## Genfromtxt to convert our csv file to array\n",
        "from numpy import genfromtxt\n",
        "## Keras.* for building my fully connected dense neural network \n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation,Flatten\n",
        "from keras.callbacks import Callback\n",
        "from keras.optimizers import SGD\n",
        "## Mean_squared_error from sklearn.metrics for calculating mean squared error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "## keras.callbacks.* for Earlystopping (modelcheckpoint is needed to load back the weights)\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YM-AHGITlwau",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True);\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z058UiCdiIHn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Iterate all csv\n",
        "def iterate_csv(path):\n",
        "    # Iterating over the raw images.\n",
        "    for filename in os.listdir(path):\n",
        "      csv = np.genfromtxt(filename, delimiter=',')\n",
        "      image = csv.reshape(128,128,3)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5CoQZxGOl9S9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Function for converting RGB images from path to LAB images.\n",
        "# path:          String, filepath of the images\n",
        "# return value:  Float array 128x128x3, The array of the images in LAB colorization\n",
        "def path2labimage(path):\n",
        "  #Opening the image from path and converting it to float array\n",
        "  raw_image_array = np.array(Image.open(path)).astype('float32')\n",
        "  #Converting RGB image to LAB\n",
        "  image_array = skcolor.rgb2lab(raw_image_array/255)\n",
        "  #Returning the float array with the LAB image values\n",
        "  return image_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bRwOLPyJhDWZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Function for normalizing the images and saving it to csv files.\n",
        "# initial_path:          String, filepath of the images\n",
        "# target_path:           String, intended folder path of csv files\n",
        "# array_average:         Int, the average value of the train dataset\n",
        "# array_std:             Int, the standard deviation value of the train dataset\n",
        "def dataset_normalized_csv(initial_path,target_path,array_average,array_std):\n",
        "    # Creating directory for the transformed dataset, if it does not exist.\n",
        "    if not os.path.exists(target_path):\n",
        "        os.makedirs(target_path)\n",
        "        \n",
        "    t1=datetime.datetime.now()\n",
        "    # Iterating over the raw images.\n",
        "    for filename in os.listdir(initial_path):\n",
        "        t2 = datetime.datetime.now()\n",
        "        if(t2.minute != t1.minute):\n",
        "          print(str(i)+'/'+str(len(os.listdir(initial_path))))\n",
        "          t1=t2\n",
        "         \n",
        "        #Converting path to LAB image\n",
        "        image_array = path2labimage(initial_path + filename)\n",
        "        #Normalizing the grayimage\n",
        "        image_array[:,:,0] = (image_array[:,:,0]-array_average)/array_std\n",
        "        #Min max scaling the color dimensions\n",
        "        image_array[:,:,1::] = image_array[:,:,1::]/255\n",
        "        #Saving to a csv files\n",
        "        np.savetxt((target_path + filename[:-3] + 'csv'), image_array.reshape((-1,3)), delimiter=\",\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JEww1VNChD8I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dataset_std(initial_path, image_size, valid_split, test_split):\n",
        "  NA = image_size*image_size;\n",
        "  NB = 0;\n",
        "  SA = 0;\n",
        "  SB = 'a';\n",
        "  a_ave = 0;\n",
        "  b_ave = 0;\n",
        "  i = 0;\n",
        "  t1=datetime.datetime.now()\n",
        "  for filename in os.listdir(initial_path):\n",
        "    \n",
        "        t2 = datetime.datetime.now()\n",
        "        if(t2.minute != t1.minute):\n",
        "          print(str(i)+'/'+str(len(os.listdir(initial_path))))\n",
        "          t1=t2\n",
        "          \n",
        "        image_array = path2labimage(initial_path + filename)\n",
        "        SA = np.std(image_array[:,:,0])\n",
        "        a_ave = np.average(image_array[:,:,0])\n",
        "        if(SB=='a'):\n",
        "          SB = SA\n",
        "          NB += NA\n",
        "          b_ave = a_ave;\n",
        "        else:\n",
        "          SB =math.sqrt(((NA-1)*(SA**2)+(NB-1)*(SB**2)+NA*NB/(NA+NB)*math.pow(a_ave-b_ave,2))/(NA+NB-1))\n",
        "          b_ave = (b_ave*NB+a_ave*NA)/(NB+NA)\n",
        "          NB += NA\n",
        "          \n",
        "     #   if( i >= int(len(os.listdir(initial_path))*(1-valid_split-test_split))):\n",
        "     #     return SB,b_ave\n",
        "        i += 1\n",
        "  return  SB,b_ave     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D2pMnKBhrcgv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for downloading the dataset.\n",
        "# url:           String, url of the zipped dataset\n",
        "# target_path:   String, intended filepath of the downloaded dataset\n",
        "def download_dataset(url, target_path):\n",
        "    # Downloading the file in chunks to avoid memory overrun.\n",
        "    r = requests.get(url, stream = True)\n",
        "    with open(target_path, 'wb') as f:\n",
        "        for chunk in r.iter_content(chunk_size=1024):\n",
        "            # Filtering out keep-alive new chunks.\n",
        "            if chunk: \n",
        "                f.write(chunk)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cX7THU6Crcg9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Function for extracting zipped dataset.\n",
        "# dataset_zipped_path:   String, filepath of the zipped dataset\n",
        "# raw_dataset_path:      String, intended folder path of raw dataset\n",
        "# return value:          String, final folder path of raw dataset\n",
        "def extract_dataset(dataset_zipped_path, raw_dataset_path):\n",
        "    # Creating directory for the raw dataset, if it does not exist.\n",
        "    if not os.path.exists(raw_dataset_path):\n",
        "        os.makedirs(raw_dataset_path)\n",
        "    \n",
        "    # Extracting dataset to the intended folder path.\n",
        "    zip_ref = zipfile.ZipFile(dataset_zipped_path, 'r')\n",
        "    zip_ref.extractall(raw_dataset_path)\n",
        "    zip_ref.close()\n",
        "    \n",
        "    # Determining and returning final path of the raw dataset.\n",
        "    dirlist = os.listdir(raw_dataset_path)    \n",
        "    return raw_dataset_path + dirlist[0] + '/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YiKkFog5rchJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for grayscale check of an image. Returns True if grayscale, False if not.\n",
        "# im:            PIL.Image object, input image\n",
        "# return value:  boolean, True if grayscale, False if not grayscale\n",
        "def is_gray_scale(im):\n",
        "    w,h = im.size\n",
        "    # Generating 10 random pixel coordinate.\n",
        "    rand_pixel_array = np.zeros((10,2))\n",
        "    for i in range(10):\n",
        "        rand_pixel_array[i,:] = [random.randint(0,w-1), random.randint(0,h-1)]\n",
        "    # If all of the 10 pixels have the same values on each channels, the image is regarded grayscale.\n",
        "    for i in range(10):\n",
        "        r,g,b = im.getpixel((rand_pixel_array[i,0], rand_pixel_array[i,1]))\n",
        "        if r != g != b: return False\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tnLN6BlwrchQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for dimension check of an image. Returns True if if image has the proper dimensions (3D, 3 channels), False if not.\n",
        "# im:            PIL.Image object, input image\n",
        "# return value:  boolean, True if image has the proper dimensions (3D, 3 channels), False if not\n",
        "def has_proper_dim(im):\n",
        "    # Get the image data to numpy array.\n",
        "    im_array = np.array(im)\n",
        "    shape = im_array.shape\n",
        "    # The image shall have 3 dimensions, and 3 channels.\n",
        "    if((len(shape) != 3) or (shape[2] != 3)):\n",
        "        return False\n",
        "    else:\n",
        "        return True    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a0HC9YGKrche",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for making the images to 1:1 ratio, and resizing them to the target size.\n",
        "# im:           PIL.Image object, input image\n",
        "# target_size:  tuple with 2 integer element, (width, height)\n",
        "# return value: PIL.Image object, transformed image\n",
        "def crop_resize_Image(im, target_size):\n",
        "    # Taking out the image data (width,height).\n",
        "    width,height = im.size\n",
        "    # Deciding if the image is landscape or portrait.\n",
        "    if(width > height):\n",
        "        # Landscape\n",
        "        top     = 0\n",
        "        left    = int((width - height)/2)\n",
        "        bottom  = height\n",
        "        right    = width - int((width - height)/2)\n",
        "    else:\n",
        "        # Portrait.\n",
        "        top     = int((height - width)/2)\n",
        "        left    = 0\n",
        "        bottom  = height-int((height - width)/2)\n",
        "        right    = width\n",
        "    # Cropping the image to conform 1:1 ratio, the resizing to target size.\n",
        "    return im.crop((left,top,right,bottom)).resize(target_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gi5IYfxjrchp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for standardizing the input array.\n",
        "# array:          numpy array, input array\n",
        "# return value:   numpy array, standardized input array\n",
        "def standardize(array):\n",
        "    # Calculating average on all elements of the array.\n",
        "    ave = np.average(array)\n",
        "    # Calculating standard deviation on all the elements of the array.\n",
        "    std = np.std(array)\n",
        "    # Standardizing the input array.\n",
        "    new_array = (array-ave)/std\n",
        "    # Returning standardized array.\n",
        "    return new_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H5eRidPZrch4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for transforming the images of the dataset to 1:1 ratio, and target size.\n",
        "# initial_path:  String, folder path of the raw dataset\n",
        "# target_path:   String, intended folder path of the transformed dataset\n",
        "# image_size:    tuple with 2 integer element, (width, height)\n",
        "def dataset_transform(initial_path, target_path, image_size):\n",
        "    # Creating directory for the transformed dataset, if it does not exist.\n",
        "    if not os.path.exists(target_path):\n",
        "        os.makedirs(target_path)\n",
        "    # Iterating over the raw images.\n",
        "    for filename in os.listdir(initial_path):\n",
        "        im = Image.open(initial_path + filename)\n",
        "        # Filtering out the grayscale images, and images with improper dimensions.\n",
        "        if((is_gray_scale(im) == False) and (has_proper_dim(im) == True)):\n",
        "            # Making the images to 1:1 ratio, and resizing them to the target size.\n",
        "            im = crop_resize_Image(im,(128,128))\n",
        "            # Saving the images to the target directory.\n",
        "            im.save(target_path + filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3rKOEjBXrciD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to preprocess the given number of images from the given offset to form training and validation data.\n",
        "# dataset_path:  String, folder path of the dataset\n",
        "# train_spl:     float, proportion of training data to all data\n",
        "# valid_spl:     float, proportion of validation data to all data\n",
        "# num_imgs:      int, number of images to preprocess\n",
        "# offset:        int, image index offset in dataset folder\n",
        "# return value:  training data input, training data output, validation data input, validation data output\n",
        "def preprocess_train_valid_data(dataset_path, train_spl, valid_spl, num_imgs, offset):\n",
        "    \n",
        "    # Determine validation split when taking to account only training and validation data.\n",
        "    valid_split = valid_spl / (train_spl + valid_spl)\n",
        "    \n",
        "    # Making a list of filenames of the dataset directory.\n",
        "    filename_list = os.listdir(dataset_path)\n",
        "    # Creating an empty list for the loaded images.\n",
        "    data = []\n",
        "    # Iterating over the given number of images from the given offset in the dataset.\n",
        "    for i in range(offset, (offset + num_imgs)):\n",
        "        # Loading the actual image, then converting that to a numpy array.\n",
        "        im = k_image.img_to_array(k_image.load_img(dataset_path + filename_list[i]))\n",
        "        # Scaling the rgb pixel values from 0-255 to 0-1, to comply with the input of rgb2lab() function.\n",
        "        im = im * (1.0/255.0)\n",
        "        # Converting the image from RGB to LAB color space. The datatype of the result is casted from float64 to float32.\n",
        "        im = skcolor.rgb2lab(im).astype('float32')\n",
        "        # Appending image to the list.\n",
        "        data.append(im)    \n",
        "\n",
        "    # Creating a numpy array from the list, with float32 datatype.\n",
        "    data = np.asarray(data, dtype='float32')\n",
        "    # Selecting the first channel of the images as input, that contains the grayscale representation of the image in lAB color space.\n",
        "    X = data[:,:,:,0]\n",
        "    # Selecting the second and third channels of the images as output, they contain green–red and blue–yellow color components respectively.\n",
        "    Y = data[:,:,:,1:]\n",
        "    \n",
        "    # Standardizing input data.\n",
        "    X = standardize(X)\n",
        "    # Normalizing output data from range -128...+128 to -1...+1\n",
        "    Y = Y/128\n",
        "\n",
        "    # Selecting training and validation data separately.    \n",
        "    X_train = X[0:int(num_imgs*(1-valid_split)),:,:]\n",
        "    Y_train = Y[0:int(num_imgs*(1-valid_split)),:,:,:]\n",
        "    X_valid = X[int(num_imgs*(1-valid_split)):,:,:]\n",
        "    Y_valid = Y[int(num_imgs*(1-valid_split)):,:,:,:]\n",
        "    \n",
        "    return X_train,Y_train,X_valid,Y_valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e_MX25K1rciR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to preprocess the given number of images from the given offset to form test data.\n",
        "# dataset_path:  String, folder path of the dataset\n",
        "# num_imgs:      int, number of images to preprocess\n",
        "# offset:        int, image index offset in dataset folder\n",
        "# return value:  test data input, test data output\n",
        "def preprocess_test_data(dataset_path, num_imgs, offset):\n",
        "    \n",
        "    # Making a list of filenames of the dataset directory.\n",
        "    filename_list = os.listdir(dataset_path)\n",
        "    # Creating an empty list for the loaded images.\n",
        "    data = []\n",
        "    # Iterating over the given number of images from the given offset in the dataset.\n",
        "    for i in range(offset, min(offset + num_imgs,len(filename_list))):\n",
        "        # Loading the actual image, then converting that to a numpy array.\n",
        "        im = k_image.img_to_array(k_image.load_img(dataset_path + filename_list[i]))\n",
        "        # Scaling the rgb pixel values from 0-255 to 0-1, to comply with the input of rgb2lab() function.\n",
        "        im = im * (1.0/255.0)\n",
        "        # Converting the image from RGB to LAB color space. The datatype of the result is casted from float64 to float32.\n",
        "        im = skcolor.rgb2lab(im).astype('float32')\n",
        "        # Appending image to the list.\n",
        "        data.append(im)\n",
        "\n",
        "    # Creating a numpy array from the list, with float32 datatype.\n",
        "    data = np.asarray(data, dtype='float32')\n",
        "    # Selecting the first channel of the images as input, that contains the grayscale representation of the image in lAB color space.\n",
        "    X_test = data[:,:,:,0]\n",
        "    # Selecting the second and third channels of the images as output, they contain green–red and blue–yellow color components respectively.\n",
        "    Y_test = data[:,:,:,1:]\n",
        "    \n",
        "    # Standardizing input data.\n",
        "    X_test = standardize(X_test)\n",
        "    # Normalizing output data from range -128...+128 to -1...+1\n",
        "    Y_test = Y_test/128\n",
        "    \n",
        "    return X_test, Y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V3ir1wqircig",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for converting RGB image array to grayscale image array.\n",
        "def img_grayscale(imageArray):\n",
        "    \n",
        "    imgArr= np.empty([1])\n",
        "    for image in imageArray:\n",
        "        pil_imgray = image.convert('LA')\n",
        "        img = np.array(list(pil_imgray.getdata(band=0)), int)\n",
        "        img.shape = (pil_imgray.size[1], pil_imgray.size[0])\n",
        "        imgArr=np.append(imgArr,img)\n",
        "    imgArr = np.delete(imgArr,0)\n",
        "    imgArr.shape = (len(imageArray),pil_imgray.size[1], pil_imgray.size[0])\n",
        "    return imgArr;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sbBnLINnrciu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function for making visualization for the images.\n",
        "# transformed_dataset_path   String, folder path of transformed dataset\n",
        "# width                      int, number of images on one edge\n",
        "# size                       int, size of 1:1 ratio image\n",
        "def image_mosaic(transformed_dataset_path, width, size):\n",
        "    \n",
        "    # Making a list of filenames of the dataset directory.\n",
        "    filename_list = os.listdir(transformed_dataset_path)\n",
        "    # Creating an empty list for the loaded images.\n",
        "    imagearray = []\n",
        "    # Loading the first width*width number of images, and appending them to the list.\n",
        "    for i in range(width*width):\n",
        "        im = Image.open(transformed_dataset_path + filename_list[i])\n",
        "        imagearray.append(im)\n",
        "        \n",
        "    # Initializing canvas.\n",
        "    canvas = np.ones((size*width,size*width*2,3));\n",
        "    # Resizing images.\n",
        "    for i in range(len(imagearray)):\n",
        "        imagearray[i]=imagearray[i].resize((size,size),Image.ANTIALIAS)\n",
        "    # Making GrayImages.\n",
        "    grayimage=img_grayscale(imagearray)\n",
        "    # Writing the RGB images to the canvas right side.\n",
        "    for i in range(width):\n",
        "        for j in range(width):\n",
        "            canvas[i*size:(i+1)*size,j*size:(j+1)*size,0::]=np.array(imagearray[i+width*j])\n",
        "    # Writing grayscale images to the canvas left side.\n",
        "    for i in range(width):\n",
        "        for j in range(width,2*width):\n",
        "            canvas[i*size:(i+1)*size,j*size:(j+1)*size,0]=np.array(grayimage[i+width*(j-width)])\n",
        "            canvas[i*size:(i+1)*size,j*size:(j+1)*size,1]=np.array(grayimage[i+width*(j-width)])\n",
        "            canvas[i*size:(i+1)*size,j*size:(j+1)*size,2]=np.array(grayimage[i+width*(j-width)])\n",
        "            \n",
        "    # Displaying the mosaic.\n",
        "    canvas = canvas.astype(np.uint8);\n",
        "    mosaic = Image.fromarray(canvas)\n",
        "    mosaic.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JkIhjs2kpKvG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Specifying train, validation and test split values.\n",
        "train_split = 0.8\n",
        "valid_split = 0.1\n",
        "test_split = 0.1\n",
        "# Specifying intended filepath for the dataset to be downloaded.\n",
        "dataset_zipped_path = os.getcwd() + '/zipped_dataset.zip'\n",
        "# Specifying intended folder path of raw dataset.\n",
        "raw_dataset_path = os.getcwd() + '/raw_dataset/'\n",
        "# Specifying intended folder path of transformed dataset.\n",
        "transformed_dataset_path = '/content/gdrive/My Drive/DeepLearning/Images' + '/transformed_dataset/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "axQbTLTLrci4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Specifying input image size.\n",
        "image_size = (128,128)\n",
        "# Specifying maximal number of images that can be loaded into memory at one time.\n",
        "max_loaded_imgs_num = 1000\n",
        "\n",
        "print('Start zip file download. '+str(datetime.datetime.now()))\n",
        "# Downloading zipped dataset.\n",
        "download_dataset('https://datasets.figure-eight.com/figure_eight_datasets/open-images/test_challenge.zip', dataset_zipped_path)\n",
        "print('Zip file downloaded. '+str(datetime.datetime.now()))\n",
        "\n",
        "print('Start images extraction. '+str(datetime.datetime.now()))\n",
        "# Extracting zipped dataset to the intended folder path.\n",
        "raw_dataset_path = extract_dataset(dataset_zipped_path, raw_dataset_path)\n",
        "print('Images are extracted. '+str(datetime.datetime.now()))\n",
        "\n",
        "print('Start dataset transforming. '+str(datetime.datetime.now()))\n",
        "# Transforming raw dataset to the proper format. \n",
        "dataset_transform(raw_dataset_path, transformed_dataset_path, (128,128))\n",
        "print('Dataset is transformed. '+str(datetime.datetime.now()))\n",
        "\n",
        "# Visualization dataset by displaying a mosaic.\n",
        "image_mosaic(transformed_dataset_path, 10, 64)\n",
        "\n",
        "# Determining the length of the dataset.\n",
        "len_dataset = len(os.listdir(transformed_dataset_path))\n",
        "\n",
        "# Since the whole dataset cannot be loaded to the memory at the same time, the training of the network will be\n",
        "# executed in cycles. In each cycle (except the last) a predetermined number of images (max_loaded_imgs_num) are\n",
        "# loaded to the memory, where they are preprocessed and split to training and validation datasets. Then epoch\n",
        "# number of training and validation phase are executed on the neural network with these datasets. The images\n",
        "# loaded to the memory are different in every cycle.\n",
        "\n",
        "# Determining the combined length of the training and validation data.\n",
        "len_train_val_set = (int)(len_dataset*(train_split+valid_split))\n",
        "# Determinde number of (training + validation) cycles.\n",
        "num_cycles_train_val = (int)(len_train_val_set/max_loaded_imgs_num) + 1\n",
        "# Determine the length of the last section of (training + validation) data.\n",
        "len_last_section_train_val = len_train_val_set - max_loaded_imgs_num * (num_cycles_train_val - 1)\n",
        "\n",
        "# Since the whole dataset cannot be loaded to the memory at the same time, the network evaluation on test data\n",
        "# will be executed in cycles. In each cycle (except the last) a predetermined number of images (max_loaded_imgs_num)\n",
        "# are loaded to the memory, where they are preprocessed, forming the test dataset. Then the test phase is executed\n",
        "# on the neural network with this dataset. The images loaded to the memory are different in every cycle.\n",
        "\n",
        "# Determining the length of the test data.\n",
        "len_test_set = len_dataset - len_train_val_set\n",
        "# Determinde number of test cycles.\n",
        "num_cycles_test = (int)(len_test_set/max_loaded_imgs_num) + 1\n",
        "# Determine the length of the last section of test data.\n",
        "len_last_section_test = len_test_set - max_loaded_imgs_num * (num_cycles_test - 1)\n",
        "\n",
        "# Execution of (num_cycles_train_val-1) training cycle.\n",
        "#for i in range(num_cycles_train_val-1):\n",
        "#    (X_train,Y_train,X_valid,Y_valid) = preprocess_train_valid_data(transformed_dataset_path, train_split, valid_split, max_loaded_imgs_num, i * max_loaded_imgs_num)\n",
        "    # training the model\n",
        "\n",
        "# If there are images left in the last section, execute last training cycle.\n",
        "#if(len_last_section_train_val != 0):    \n",
        "#    (X_train,Y_train,X_valid,Y_valid) = preprocess_train_valid_data(transformed_dataset_path, train_split, valid_split, len_last_section_train_val, (num_cycles_train_val - 1) * max_loaded_imgs_num)\n",
        "    # training the model\n",
        "\n",
        "# Execution of (num_cycles_test-1) test cycle.\n",
        "#for i in range(num_cycles_test-1):\n",
        "#    (X_test, Y_test) = preprocess_test_data(transformed_dataset_path, max_loaded_imgs_num, len_train_val_set + i * max_loaded_imgs_num )\n",
        "    # elvaluation of test data\n",
        "\n",
        "# If there are images left in the last section, execute last test cycle.\n",
        "#if(len_last_section_test != 0):\n",
        "#    (X_test, Y_test) = preprocess_test_data(transformed_dataset_path, len_last_section_test, len_train_val_set + (num_cycles_test - 1) * max_loaded_imgs_num )\n",
        "    # evaluation of test data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2TE2qi9CTp0h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datetime.datetime.now()\n",
        "print('Start calculating average and std value on train data. '+str(datetime.datetime.now()))\n",
        "std,ave = dataset_std(transformed_dataset_path,128,valid_split,test_split)\n",
        "print('Calculated average and std value, '+str(datetime.datetime.now()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z0XF7JXze66O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print('Start transforming images to csv files. '+str(datetime.datetime.now()))\n",
        "csv_dataset_path ='/content/gdrive/My Drive/DeepLearning/Image' + '/csv_dataset/'\n",
        "dataset_normalized_csv(transformed_dataset_path,csv_dataset_path,ave,std)\n",
        "print('Transformed images to csv files. '+str(datetime.datetime.now()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dnhyvOT35cAm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jdEjTfq8rlAa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Class TrainingHistory\n",
        "class TrainingHistory(Callback):\n",
        "  def on_train_begin(self, logs={}):\n",
        "    self.losses=[]\n",
        "    self.valid_losses =[]\n",
        "    self.accs = []\n",
        "    self.valid_accs = []\n",
        "    self.epoch=0\n",
        "    \n",
        "  def on_epoch_end(self, epoch,logs={}):\n",
        "    self.losses.append(logs.get('loss'))\n",
        "    self.valid_losses.append(logs.get('val_loss'))\n",
        "    self.accs.append(logs.get('acc'))\n",
        "    self.valid_accs.append(logs.get('val_acc'))\n",
        "    self.epoch += 1\n",
        "#Initializing the history  \n",
        "history = TrainingHistory()\n",
        "\n",
        "#Defining the earlystopping\n",
        "es = EarlyStopping(patience=10, verbose=1)\n",
        "mcp = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CZBL8XSxuDRj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential();\n",
        "model.add(Conv2D(data_format=\"channels_first\",input_shape=(128, 128,1), filters=32, kernel_size=(3, 3), strides=1, padding='same', activation='linear', use_bias=False))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Conv2D(data_format=\"channels_first\",filters=32, kernel_size=(3, 3), strides=1, padding='same', activation='linear', use_bias=False))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Conv2D(data_format=\"channels_first\",filters=32, kernel_size=(3, 3), strides=1, padding='same', activation='linear', use_bias=False))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "\n",
        "model.add(Conv2D(data_format=\"channels_first\",filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='linear', use_bias=False))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Conv2D(data_format=\"channels_first\",filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='linear', use_bias=False))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Conv2D(data_format=\"channels_first\",filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='linear', use_bias=False))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "\n",
        "model.add(Conv2D(data_format=\"channels_first\",filters=128, kernel_size=(3, 3), strides=1, padding='same', activation='linear', use_bias=False))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Conv2D(data_format=\"channels_first\",filters=128, kernel_size=(3, 3), strides=1, padding='same', activation='linear', use_bias=False))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Conv2D(data_format=\"channels_first\",filters=128, kernel_size=(3, 3), strides=1, padding='same', activation='linear', use_bias=False))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "\n",
        "model.add(Conv2D(data_format=\"channels_first\",filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='linear', use_bias=False))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Conv2D(data_format=\"channels_first\",filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='linear', use_bias=False))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Conv2D(data_format=\"channels_first\",filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='linear', use_bias=False))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='linear'))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(1024, activation='linear'))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(Dense(2*128*128, activation='linear'))\n",
        "model.add(Activation('sigmoid'))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vd-wkdfOuGv3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sgd=SGD(lr=1, momentum=0.09, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "\n",
        "hst = model.fit(inputArray, outputArray.reshape((-1,2*128*128)),\\\n",
        "         batch_size=1,\n",
        "         epochs=1,\n",
        "         verbose=2,\n",
        "         validation_data=(inputArrayVal,outputArrayVal.reshape((-1,2*128*128))),\n",
        "         callbacks=[mcp, es, history],\n",
        "         shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Ngak2lXuKe4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds=model.predict(inputArrayT).reshape((2,128,128,2))*255\n",
        "visual=np.empty((2,128,128,3))\n",
        "visual[:,:,:,0]=npImageArrayT[:,:,:,0]\n",
        "visual[:,:,:,1::]=preds-127\n",
        "plt.subplot(2,1,1)\n",
        "plt.imshow(skcolor.lab2rgb(visual[0]))\n",
        "plt.subplot(2,1,2)\n",
        "plt.imshow(imageArrayt[0])\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h31DKQZbuS10",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,3))\n",
        "plt.title(\"Hiba mértéke a tanítás során\")\n",
        "plt.plot(np.arange(history.epoch), history.losses, color='g',  \n",
        "         label=\"Hiba a tanító adatokon\")\n",
        "plt.plot(np.arange(history.epoch), history.valid_losses, color='r',\n",
        "         label=\"Hiba a validációs adatokon\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel(\"Epochok száma\")\n",
        "plt.ylabel(\"Hiba\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5tst0pQS6tj1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}